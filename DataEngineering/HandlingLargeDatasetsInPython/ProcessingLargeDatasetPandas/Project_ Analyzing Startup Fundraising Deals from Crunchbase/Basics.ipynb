{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Analyzing Startup Fundraising Deals from Crunchbase\n",
    "\n",
    " In this guided project, we'll practice using different ways to work with larger datasets in pandas to analyze startup investments from Crunchbase.com.\n",
    " \n",
    "Crunchbase is a website that crowdsources information on the fundraising rounds of many startups. The Crunchbase user community submits, edits, and maintains most of the information in Crunchbase.\n",
    "\n",
    "Throughout this guided project, we'll practice working with different memory constraints. In this step, let's assume we only have 10 megabytes of available memory. While crunchbase-investments.csv consumes 10.3 megabytes of disk space, we know from earlier missions that pandas often requires 4 to 6 times amount of space in memory as the file does on disk (especially when there's many string columns).\n",
    "\n",
    "solution:\n",
    "https://github.com/dataquestio/solutions/blob/master/Mission167Solutions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the data set contains over 50,000 rows, you'll need to read the data set into dataframes using 5,000 row chunks to ensure that each chunk consumes much less than 10 megabytes of memory.\n",
    "\n",
    "\n",
    "Across all of the chunks, become familiar with:\n",
    "Each column's missing value counts\n",
    "Each column's memory footprint\n",
    "The total memory footprint of all of the chunks combined\n",
    "Which column(s) we can drop because they aren't useful for analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mv_by_col = {}\n",
    "memory_by_col = {}\n",
    "total_memory = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's get familiar with the column types before adding the data into SQLite.\n",
    "\n",
    "Identify the types for each column.\n",
    "Identify the numeric columns we can represent using more space efficient types.\n",
    "For text columns:\n",
    "Analyze the unique value counts across all of the chunks to see if we can convert them to a numeric type.\n",
    "See if we clean clean any text columns and separate them into multiple numeric columns without adding any overhead when querying.\n",
    "Make your changes to the code from the last step so that the overall memory the data consumes stays under 10 megabytes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to load each chunk into a table in a SQLite database so we can query the full data set.\n",
    "\n",
    "Create and connect to a new SQLite database file.\n",
    "Expand on the existing chunk processing code to export each chunk to a new table in the SQLite database.\n",
    "Query the table and make sure the data types match up to what you had in mind for each column.\n",
    "Use the !wc IPython command to return the file size of the database.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is in SQLite, we can use the pandas SQLite workflow we learned in the last mission to explore and analyze startup investments. Remember that each row isn't a unique company, but a unique investment from a single investor. This means that many startups will span multiple rows.\n",
    "\n",
    "Use the pandas SQLite workflow to answer the following questions:\n",
    "What proportion of the total amount of funds did the top 10% raise? What about the top 1%? Compare these values to the proportions the bottom 10% and bottom 1% raised.\n",
    "Which category of company attracted the most investments?\n",
    "Which investor contributed the most money (across all startups)?\n",
    "Which investors contributed the most money per startup?\n",
    "Which funding round was the most popular? Which was the least popular?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some ideas for further exploration:\n",
    "\n",
    "Repeat the tasks in this guided project using stricter memory constraints (under 1 megabyte).\n",
    "Clean and analyze the other Crunchbase data sets from the same GitHub repo.\n",
    "Understand which columns the data sets share, and how the data sets are linked.\n",
    "Create a relational database design that links the data sets together and reduces the overall disk space the database file consumes.\n",
    "Use pandas to populate each table in the database, create the appropriate indexes, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
